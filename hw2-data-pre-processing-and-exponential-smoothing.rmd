---
title: 'Homework 2: Data Pre-processing & Exponential Smoothing'
author: "Group 4 - Subhalaxmi Rout, Kenan Sooklall, Devin Teran, Christian Thieme, Leo Yi"
date: "6/12/2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mlbench)
library(inspectdf)
library(mice)
library(corrplot)
library(caret)
library(VIM)
library(fma)
```

## Applied Predictive Modeling Chapter 3 - Data Pre-processing

### 3.1

**The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. The data can be accessed via:**

```{r}
data(Glass)
glimpse(Glass)
```

**$(a)$ Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.**

Since our predictors are continuous variables, we'll use histograms to understand their distributions:

```{r fig.height=6, fig.width=10}

inspectdf::inspect_num(Glass) %>% 
  show_plot()
```

We note the following:

-   Al: Appears to have a fairly normal distribution, slightly right skewed
-   Ba: Over 75% of this distribution is 0 values. We note some potential outliers that will need to be investigated.
-   Ca: The distribution has two values that make up over 60% of the distribution. The distribution is right skewed.
-   Fe: Similar to Ba, 65% of the values are 0. We note the presence of potential outliers.
-   K: Two values make up \~90% of the distribution. We note the presence of potential outliers.
-   Mg: The distribution is bi-modal, which may indicate sub-populations. 20% of the distribution is 0s. The remainder of the distribution is left skewed.
-   Na: Right skewed.Values before 12.5 are rare.
-   Rl: The distribution is right skewed. Values before 1.515 are rare.
-   Si: The distribution is left skewed. Values after 74 are rare.

Now we turn our attention to the relationship between predictors. We'll review a correlation plot to assess the relationship between predictors: 

```{r fig.height=8, fig.width=8}
correlation <- cor(Glass %>% select(-c('Type')))
corrplot.mixed(correlation, tl.col = 'black', tl.pos = 'lt')
```

Correlation plots can be deceiving if used without looking at the visual relationship between the data. Let's create some scatter plots of the relationships as well. 

```{r fig.height=10, fig.width=10, warning=TRUE}
pairs(Glass %>% dplyr::select_if(is.numeric))
```

We note very few meaningful relationships between the predictors despite what the correlation plot shows:

-   There appears to be a loosely negative relationship between Ri and Al and Ri and Si. There appears to be a positive relationship between Ri and Ca.


**$(b)$ Do there appear to be any outliers in the data? Are any predictors skewed?**

We noted skew and outliers in our discussion above of the histograms.

**$(c)$ Are there any relevant transformations of one or more predictors that might improve the classification model?**

This data would benefit from centering and scaling as they are all on different scales. Additionally, several of the skewed variables may benefit from a log or similar transformation. We recommend using the Box-Cox approach to determine the necessary transformation for each of the skewed variables.

### 3.2

**The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes. The data can be loaded via:**

```{r}
data("Soybean")
glimpse(Soybean)
```

**$(a)$ Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?**

We'll look at a chart for all of the factor variables to see the % of the most common level within the factor:

```{r warning=FALSE}
# factor analysis. visualize most common factors
inspectdf::inspect_imb(Soybean) %>% show_plot()
```

In looking at the chart, we can see that near zero variance may be an issue for several features within our dataset. For example, mycelium, sclerotia, leaves, and int.discolor all have 85%+ of their values with the same level. As discussed in the reading, this can be problematic. Let's run the `nearZeroVar` function from the `caret` package to see which variables meet the criteria:

```{r}
nzv <- caret::nearZeroVar(Soybean, names = TRUE, saveMetrics = TRUE)
nzv %>% filter(nzv == TRUE)
```

We should consider removing the 3 variables above before modeling.

**$(b)$ Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?**

```{r}
visdat::vis_miss(Soybean, sort_miss = TRUE)
```

9.5% of the data have NAs. Several of the predictors have more than 15% of their values missing such as hail, sever, seed.tmt, and lodging. Additionally, the missing values appear to have some pattern as the nulls are not random.

```{r}
Soybean %>%
  filter(!complete.cases(.)) %>% 
  count(Class) %>% 
  mutate('% missing' = n/dim(Soybean)[1]) %>% 
  arrange(desc(`% missing`))
```

From the missing 18% of our data, we can see that more than half of it is related to the phytophthora-rot class. The percentages show us what percent of the total data is missing, but lets investigate how much of each class is missing: 

```{r}
complete <- Soybean %>%
  count(Class) %>%
  rename(total = n)

missing <- Soybean %>%
  filter(!complete.cases(.)) %>% 
  count(Class) %>%
  rename(missing = n)

final <- missing %>% 
  left_join(complete) %>% 
  mutate('% missing' = missing/total)

final
  
```

It appears that several classes are missing at least one cell in each observation (Albeit, they may be a fairly small part of the population).Phytophthora-rot is missing data in 77% of its observations and is a larger percentage of the total population. 

**$(c)$ Develop a strategy for handling missing data, either by eliminating predictors or imputation.**

In looking at the the proportions and combinations of missing values as well as understanding that half of our missing values come from one class, we have chosen to impute missing values using predictive mean matching (pmm). Predictive mean matching calculates the predicted value for our target variable, and, for missing values, forms a small set of "candidate donors" from the complete cases that are closest to the predicted value for our missing entry. Donors are then randomly chosen from candidates and imputed where values were once missing. This method is similar in theory to knn, however, it is often better at imputation. We can utilize pmm through the `mice` library.

```{r message=FALSE, warning=FALSE}
Sb <- mice(data = Soybean, m = 1, method = "pmm", printFlag=F, seed = 500)
Sb <- mice::complete(Sb, 1)

colSums(is.na(Sb))
```

In the output above, we can see that there are no longer missing values in our dataset.

## Forecasting: Principles and Practice - Exponential smoothing

### 7.1

**Consider the `pigs` series --- the number of pigs slaughtered in Victoria each month.**

```{r}
pigs <- fma::pigs
```

**$(a)$ Use the `ses()` function in R to find the optimal values of $\alpha$ and $l_0$, and generate forecasts for the next four months.**

Optimal values can be found by looking at the `model` element of the forecast object.

```{r}
simple <- ses(pigs, h = 4)
simple$model
```

In the code above, we generated the next 4 months of predictions. We can access them by calling `mean` on our forecast object `simple`.

```{r}
simple$mean
```

**$(b)$ Compute a 95% prediction interval for the first forecast using $yÂ±1.96s$ where s is the standard deviation of the residuals. Compare your interval with the interval produced by R.**

```{r}
prediction <- simple$mean[1] 
pred_interval <- 1.96 * (sd(simple$residuals))
upper <- prediction + pred_interval
lower <- prediction - pred_interval

print(c(lower, upper))
```

We calculated the prediction interval in the code chunk above and got an interval of (78,679.97, 118,952.84). We'll now look at the interval calculated by the forecast object:

Upper bound:

```{r}
simple$upper[1,2]
```

Lower bound:

```{r}
simple$lower[1,2]
```

We can see that the interval is very close. It's within \~68 on each side.

### 7.2

**Write your own function to implement simple exponential smoothing. The function should take arguments y (the time series), alpha (the smoothing parameter alpha) and level (the initial level lambda). It should return the forecast of the next observation in the series. Does it give the same forecast as `ses()`?**

```{r}

simple_exponential_smoothing <- function(y, alpha, level) {
  
  current_level <- level
  
  for (i in 1:length(y)){
    current_level <-  (alpha * y[i]) + (1 - alpha) * current_level
  }
  
  return(current_level)
  
}

simple_exponential_smoothing(pigs, simple$model$par[[1]], simple$model$par[[2]])

```

Our function returns 98,816.41. Now let's see what R returns:

```{r}
simple$mean[1]
```

The values are identical.

### 7.3

**Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the `optim()` function to find the optimal values of alpha and lambda 0. Do you get the same values as the `ses()` function?**

*This [article](https://www.magesblog.com/post/2013-03-12-how-to-use-optim-in-r/) was helpful in solving this question.*

We'll reconfigure our function to calculate the sum of squared errors and then use the `optim` function to find the optimal values of alpha and lambda:

```{r}
sum_of_squared_error <- function(y, par = c(alpha, level)){
  e <- 0
  SSE <- 0
  alpha <- par[1]
  current_level <- par[2]
  return_value <- current_level
  
  for (i in 1:length(y)){
    e <- y[i] - current_level
    SSE <- SSE + e^2
    
    current_level <- alpha * y[i] + (1 - alpha) * current_level
  }
  return(SSE)
}

results <- optim(y = pigs, par = c(0.5, pigs[1]),  fn = sum_of_squared_error)
results$par
```

Now, that we have our optimized values, let's compare these to what the `ses` function returns:

```{r}
simple$model$par
```
Our values are not exactly the same, but they are very, very close. 